{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a61350e8-30e9-45a6-a82d-7368456ea3f7",
   "metadata": {},
   "source": [
    "# Cluster Computing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e6f443-eaf7-4722-8da3-0460d4ede506",
   "metadata": {},
   "source": [
    "Physics and astronomy often invove large datasets or a large number of computations or both, for which simply running code on your personal machines will not suffice. \n",
    "You could need more firepower (eg. more cores), more memory or just different processors (eg. GPUs). This is where computer clusters come in. You take your code, stick it on a cluster and submit jobs to be run on the cluster remotely. Here, we'll go over the basics of how to do that. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e296405-7f4e-4a63-a4ed-564c73b624a4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Connecting to a cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1530327a-0b7d-4a35-a91b-72de4345684c",
   "metadata": {},
   "source": [
    "In [Section 2](https://github.com/jeffiuliano/Penn-Summer-Computing-Training/blob/main/2_ssh_and_scp/SSH_SCP_Workbook.ipynb), we learnt to securely interact with a non-local machine. To connect to most clusters, we will use SSH as described there. For example, for those of you using *marmalade*, you will do "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221ae077-3e1c-4075-b605-fb43f8a1e731",
   "metadata": {},
   "source": [
    "`ssh username@marmalade.physics.upenn.edu`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2ab7db-f3a8-4bcc-aaa0-950e389771a2",
   "metadata": {},
   "source": [
    "- Note that to connect to *marmalade*, you must be on a Penn secured network (eg. AirPennNet wi-fi) or VPN into Penn. For using the Penn VPN, you need a PennKey (UPenn username and account) and associated account setup. For details of how to VPN into Penn, go [here](https://www.isc.upenn.edu/how-to/university-vpn-getting-started-guide). \n",
    "- Note also that *marmalade* access is granted by your PI. Please bother them to get access. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3851fef5-b11c-4be4-bbd5-d0ad5f6613f2",
   "metadata": {},
   "source": [
    "This will then prompt you for a password. The password for your account should be the same as the password associated with your PennKey. Remember that your terminal will not indicate that you are typing while you enter your password. If you are successful, the terminal will print out something like:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0283683a-183c-4344-a255-b0f8c0c1cb8b",
   "metadata": {},
   "source": [
    "`Last login: Wed May 4 15:58:52 2022 from xx.xxx.xxx.xxx`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8023ef92-b8fb-4da5-88a9-b666102b40bc",
   "metadata": {},
   "source": [
    "You are now on the **head node / login node** on *marmalade*. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0720fe8-307b-43b3-8f78-f0fd66cbb7ac",
   "metadata": {},
   "source": [
    "### Head / Login Nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587f2db3-e2aa-4dbe-a8a5-4dbd005838cd",
   "metadata": {},
   "source": [
    "#### What is a head node and why will everyone get mad at you if you mess with it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd03c78-ec1f-4cf1-8c0b-e1699c0fbf1d",
   "metadata": {},
   "source": [
    "Most clusters are set up such that you land on a *head or login node* when you connect to it. This is not the location where actual jobs are run, this node will not perform your computations. It's purpose is to facilitate those computations being run on a *compute node*. **Do not use the login node to run scripts.** You might wind up preventing other people from logging in by consuming resources. This node will not have enough memory to support large jobs. Etc etc. **This is how you make everyone in your research group and beyond angry with you.** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe48968-c052-4319-94a5-351e3c45893b",
   "metadata": {},
   "source": [
    "Things it is okay to use the login node for:\n",
    "- view and edit scripts\n",
    "- view output\n",
    "- perform git actions \n",
    "- submit and managae jobs\n",
    "- spy on who else is using the cluster resources "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a9f1e6-6805-4430-b8f9-0619fd2cae39",
   "metadata": {},
   "source": [
    "Things I will yell at you for using the login node for:\n",
    "- running jobs\n",
    "- debugging scripts\n",
    "- I'll probably add some more things here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310c56d4-46d1-4fa3-b3e4-844cdeb80379",
   "metadata": {},
   "source": [
    "Grey area:\n",
    "- managing your python environment\n",
    "- installing other packages \n",
    "- installing code "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68ef391-dd63-4382-b07b-78779eb4fa17",
   "metadata": {},
   "source": [
    "Note that for the \"grey area\" points, it's still better to perform these actions on a compute node if you can, because ultimately, those are the nodes that will be running your jobs, not your login node. For example, *marmalade* has AMD nodes as well as Intel nodes. Use the specific compute nodes that you will run your jobs on to compile your code, because there are some differences between the two types of nodes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f58324-3dbf-40f2-b72e-f2083fe10cbe",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Cores vs. Nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d19585-aaed-488b-a5ac-8038232d2545",
   "metadata": {},
   "source": [
    "**Nodes** are effectivly a self-contained CPU. This has some memory, input/output and storage. This also has processors. The processors are sometimes referred to as **cores**, but usually, each processor is made up of a ~couple cores. \n",
    "\n",
    "The cores then share everything the node has - they share memory, I/O and storage. But for parallelisable code, you can parallelise across these cores and make them simultaneously run tasks. \n",
    "\n",
    "For *marmalade*, \n",
    "- astro has 3 AMD nodes which each have 64 processors or a total of 128 cores\n",
    "- CM group has Intel nodes, you should bother them for their details \n",
    "- CM also has some GPUs "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b7b13d-112c-4e73-b270-7d59af970b53",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Home "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881f5266-781c-43fe-8364-c910136bd1e2",
   "metadata": {},
   "source": [
    "The login node will take you to your home directory. This is where you can install your code, edit your `.bashrc`, direct your output. The home directories on *marmalade* are backed up every 24h. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b28797-3d98-491e-b870-fbad958ea5d9",
   "metadata": {},
   "source": [
    "### Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef8b186-d177-4db3-a3ce-d507cc18e192",
   "metadata": {},
   "source": [
    "Depending on the cluster, a separate location is preferred for runtime output called *scratch*. This has faster input/output than the *home* directory. Depending on how often you need to read/write during a job, it's better to send stuff to scratch as opposed to home. \n",
    "\n",
    "For *marmalade*, the scratch directories are attached to the compute nodes. So if your job was running on `node11` say, then the scratch in node11 will hold your output. The jobs I run on *m* are basic MCMCs, don't need fast I/O, so I just write to home. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b4b6a0-7d44-496c-aca5-f9b474fd9ee9",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7146a3-27c1-4b16-97c1-4e18e2b00ee3",
   "metadata": {},
   "source": [
    "Again, depending on the setup of the cluster, sometimes a data directory is provided per group to share and store large quantities of data. \n",
    "\n",
    "Astro on *m* doesn't have this, but CM does I think. Bother your CM grad students / postdocs to learn more. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3804f67-dcf1-4113-aad8-a68b777b82e4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Cluster set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90fcb91-b84c-4dc7-b234-8fce4d4d4151",
   "metadata": {},
   "source": [
    "Now that you're logged in, how do you go about setting up you code and ensuring it runs correctly? There's a few simple places to start:\n",
    "- if it's your own, personal code, you can scp it onto the cluster \n",
    "- git clone from a repository "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16067436-3c17-4e65-9742-7500ff547cb6",
   "metadata": {},
   "source": [
    "### Loading available software"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978e28be-6ad9-4c96-9800-40ad4c9104c6",
   "metadata": {},
   "source": [
    "You might also need modules to be able to run your code, for eg. an installation of C++ or fortran or python. Most clusters will have installations of these already in place, available for you to use. A good place to start to look for these is \n",
    "\n",
    "`module avail` \n",
    "\n",
    "This prints out all available pre-installed software that you can just load onto your profile on the cluster, for eg. with \n",
    "\n",
    "`module load git/xx.xx`\n",
    "\n",
    "You can then check that you indeed have access to git now with \n",
    "\n",
    "`which git`\n",
    "\n",
    "This should print some location of where the git you are using lives `...bin/git`\n",
    "\n",
    "Useful modules on *m* include a few versions of GCC, git, anaconda, valgrind ... The cluster helpdesk (marmalade-manager@sas.upenn.edu) is usually responsive and will install more software to add to the list of available modules if you make a good case for it (perhaps even if you make a bad one). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c2d521-edda-435b-8149-aeabc812b20b",
   "metadata": {},
   "source": [
    "### Saving cluster set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0493c3be-bb3c-485e-8cf3-edb0b2f508fe",
   "metadata": {},
   "source": [
    "Once you know which modules you will need and have compiled your codes based on these modules, you should save this set-up. \n",
    "Check whihc modules you currently have loaded by doing \n",
    "\n",
    "`module list`\n",
    "\n",
    "Then save these in your .bashrc or your .bash_profile to ensure they are loaded everytime you login:\n",
    "\n",
    "`module load xxx/xxx.xx`\n",
    "\n",
    "You should also save your anaconda environment. Someone else will tell you about that. This saves the version numbers of your python packages and loads to exact right ones for which you have compiled your code and know that it works. To export the package info for the active environment, do \n",
    "\n",
    "`conda env export > environment.yml`\n",
    "\n",
    "These are important programming practices and will save you SO much time when things invariably break or you have to move your work to a different cluster. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9554a99c-5a5b-4c4a-aa37-eafdcebf0bc9",
   "metadata": {},
   "source": [
    "### Source cluster set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd4ef58-ce21-492f-9f52-f446d9f225e6",
   "metadata": {},
   "source": [
    "When you login, the cluster usually sources your .bashrc and .bash_profile. This loads all your settings, as long as oyu saved them here. \n",
    "\n",
    "Some clusters are set up such that you don't need to reload these settings when you move to a compute node to run a job interactively or when you submit a job. Some aren't. \n",
    "\n",
    "For *m*, I need to source my .bashrc and .bash_profile everytime I run an interactive job, and have gotten into the habit of doing the same for submitted jobs\n",
    "\n",
    "`source .bashrc` \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb22c36d-d6d7-4100-bc2b-21f71f35a1c2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Running jobs "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43e3de7-f577-4a60-90bf-22ddbb2c1b78",
   "metadata": {},
   "source": [
    "Ideally, you'd like to test your code before try to submit a job and have to wait for it to begin, compute and possibly fail because of some bug. Clusters often have debug queues dedicated to this purpose that you can submit jobs to or directly interact with to run your code. \n",
    "\n",
    "Below I'll cover how to do both **assuming the cluster uses the SLURM queuing system.** Commands for this usually start with \"s\" and you can identify them below. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a28c369-6910-4dbd-9191-949ba43fee46",
   "metadata": {},
   "source": [
    "### Interactive sessions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8063d9-2b6a-4b84-888a-f65c9e245ad0",
   "metadata": {},
   "source": [
    "An interactive session is how you usually run code on your own machine. You run a command, wait for it to finish running, kepp getting runtime output printed to your screen, once it's finished, you run the next command. \n",
    "\n",
    "You can also run interactive sessions on clusters. The exact commands sometimes differ, but the gist is \n",
    "\n",
    "`srun -N <number of nodes requested> -c <cpus per task> -n <number of tasks> -p <name of partition / queue> -t <time in minutes> --pty bash` \n",
    "\n",
    "where you're asking SLURM to run asap (you can add a delay by specifying with more arguments) on N nodes, c cpus per task and n tasks (this is an overspecified problem, N and c or N and n or n and c are enough), on the p queue for t time and to open you a bash shell. \n",
    "\n",
    "Alternatively, you can give a similar command with an executable at the end instead of requesting a bash shell. \n",
    "\n",
    "Usually, I just want a few cores for a small time to debug. I'm astro so on *m* I use the *highcore* queue and ask for a bash shell:\n",
    "\n",
    "`srun -n 4 -p highcore -t 60 --pty bash` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e30eec6-1784-4a6a-a770-06c7de921b15",
   "metadata": {},
   "source": [
    "There's infinite more commands you can give to specify what kind of job you want to run. [Here](https://slurm.schedmd.com/srun.html) is a pretty exhaustive list of them. You can also try looking at the documentation for other clusters because as long as they too use SLURM, the commands should tranfer. \n",
    "\n",
    "NERSC should have its own documentation, but unfortunately *m* doesn't have any. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec281c86-79f8-4dd8-a988-8ab2b3150716",
   "metadata": {},
   "source": [
    "### Submitting jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b61576-2c51-4fe2-9b23-d4e02deda6ee",
   "metadata": {},
   "source": [
    "Another, usually better way to run things is to submit a job to the queuing system. The system then schedules your job, it will start without further intervention and per specification, email you when it ends. You can then go do other things and still delude yourself into believing you're being productive. I highly recommend it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb9999e-38b5-4866-825c-90ae02a2aa7e",
   "metadata": {},
   "source": [
    "#### SLURM specifications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc75e8b4-e49c-4a31-bdf6-d6bef2700802",
   "metadata": {},
   "source": [
    "To do this, we write a job submission script that specifies for example, the queue to submit to, how many nodes you want, how many cores, for how long, when to email you about the job, etc. Here's an example:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cdad741a-a138-4cce-983c-acb2669dd652",
   "metadata": {},
   "source": [
    "#!/bin/bash\n",
    "#\n",
    "#SBATCH --job-name=lcdm\n",
    "#SBATCH --output=j_lcdm_base_mcmc.txt\n",
    "#SBATCH --ntasks-per-node=6\n",
    "#SBATCH --cpus-per-task=4\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --time=1-0:00:00\n",
    "#SBATCH --partition=highcore\n",
    "#SBATCH --mail-type=ALL\n",
    "#SBATCH --mail-user=karwal@sas.upenn.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a9cf3c-2007-4523-a992-8c01f57eb2a8",
   "metadata": {},
   "source": [
    "Here, I'm doing the following:\n",
    "- specifying that this is a bash script\n",
    "- setting job name = lcdm\n",
    "- sending output to the file j_lcdm_base_mcmc.txt in the same directory as the one from which I launch the script \n",
    "- asking for 6 tasks per node\n",
    "- and 4 cpus per task. So in total, this takes up 24 cpus. \n",
    "- for the queue I submit to, 24 cpus easily fit into one node, so I don't need more than that. This also puts all my tasks on the same node, important for my specific code. \n",
    "- asking this to run for 1 day\n",
    "- on the highcore queue\n",
    "- requesting an email about everything - this includes job beginning, finishing, timing out, or failing\n",
    "- to my email address. For marmalade, I think this has to be a Penn address"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46fee0ed-e477-4150-8607-461149786079",
   "metadata": {},
   "source": [
    "Any line that begins with `#SBATCH` is read by SLURM. All others are taken to be bash commands."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5b0a31-149d-4e78-8b8a-04fdb1268fa5",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Which queue is for you? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d2c49c-be7b-454b-8841-7453e027b912",
   "metadata": {},
   "source": [
    "Which queue you submit to is important. You might request too many resources and your job will never be scheduled, you might not have permissions to submit to certain queues, etc. all which will delay science. Or make people mad. Or both! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe837404-2ee7-4d62-9685-2fa804fbc55b",
   "metadata": {},
   "source": [
    "For *marmalade*, if you're astro, you can submit to:\n",
    "- highcore (AMD nodes)\n",
    "- low_compute (Intel nodes)\n",
    "- low_gpu\n",
    "- low\n",
    "\n",
    "For CM on marmalade, please check with folks in your group. Besides the partitions of your group, you can also submit to\n",
    "- low_highcore \n",
    "\n",
    "This is just so astro folk have principal access to nodes they paid for and the same for CM. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c107d42-ce7a-4056-b24a-2eb70d382e55",
   "metadata": {},
   "source": [
    "For *NERSC*, check with your group. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d757f921-c626-482f-844c-11c04939dc3a",
   "metadata": {},
   "source": [
    "#### Job commands"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153d5a2e-1c80-4744-8d9e-12df2edc5945",
   "metadata": {},
   "source": [
    "The commands above tell SLURM *how* I want to run my job. Here's the rest of the script telling it what I actually want to run:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f8a4043d-1d33-4dfa-a474-4f9c1cdf63ec",
   "metadata": {},
   "source": [
    "cd ~\n",
    "source .bashrc\n",
    "source .bash_profile\n",
    "\n",
    "which python\n",
    "which gcc\n",
    "which mpirun\n",
    "\n",
    "cd /home/karwal/some/directory/\n",
    "\n",
    "mpirun -np 6 some_paralleliseable_python_script.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f861c89-7d53-454c-b144-6420de552ea6",
   "metadata": {},
   "source": [
    "Let's use knowledge you've hopefully built over this tutorial and see that here I'm doing:\n",
    "- change directory to my home directory. That's where `~` takes you\n",
    "- source my bashrc and bash_profile. This sets gets all necessary modules, sets my python environment, makes sure anything that needs to be on my PATH is added\n",
    "- then for redundancy and to debug, I check which python, gcc and mpirun the programme will call. If these are different from what I expect, one or more of my environment variables didn't set correctly\n",
    "- change directory to the one I actually want to run my script in \n",
    "- run my parallel script. Note that I'm asking mpirun to launch 6 processes here, this matches what I told SLURM - that I want 6 tasks per node and 1 node = 6x1=6 tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0860a3-8a4e-4704-911d-5bc6b6186c2c",
   "metadata": {},
   "source": [
    "Remember, the job output file that we specified earlier will be in whatever location you submitted your job script from, not in the directories `~` nor in `/home/karwal/some/directory/`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4d930c-3c1a-4210-8e87-aa62a89ad6ca",
   "metadata": {},
   "source": [
    "So now we have our job script. Let's save this in some file `job_script.sh`.  We submit simply by \n",
    "\n",
    "`sbatch job_script.sh`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7461c506-3b9f-4fdd-90d8-cfe05aaa5212",
   "metadata": {},
   "source": [
    "That's it. The job is submitted. SLURM will take note of the resources you have requested and will allocate them when it can. Your job will begin, you'll get an email about it beginning and again when it finishes (or crashes) (or runs out of time). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a466bf-4f3c-4dd7-8778-6130a76a95af",
   "metadata": {},
   "source": [
    "#### Submitting batch jobs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b806cbfb-b48b-481e-9dd1-99bbd3f8927f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b3cd0e2c-4722-4c38-be7e-7115431bd984",
   "metadata": {},
   "source": [
    "## Useful SLURM commands"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1746a8-045b-4a29-a5de-84ca761ee8e2",
   "metadata": {},
   "source": [
    "I have hopefully driven the point home that a cluster is a community resource. If you abuse that resource or don't follow rules, people will get mad at you. \n",
    "And what do you do to make them less mad? You cancel your offending jobs! \n",
    "\n",
    "`scancel <job ID number>` will terminate that job. \n",
    "\n",
    "`sinfo` tells you about what resources are in use on the cluster. Nodes can be:\n",
    "- alloc for completely allocated \n",
    "- mix for some allocated and some free cores on the node \n",
    "- drain for a node being shut down for maintainence, but the jobs on the node will finish first \n",
    "- down for a node out of commision and\n",
    "- idle for free nodes! That await a purpose in life! And that purpose is SCIENCE! \n",
    "\n",
    "`squeue` can be used to get info on jobs currerntly running on the cluster. You can add optional arguments to this to get more detailed info, for example \n",
    "\n",
    "`squeue -u username` tells you what jobs that user currently has running. \n",
    "\n",
    "I usually define a new command in my .bash_profile as \n",
    "\n",
    "`alias sqme=\"squeue -u karwal\"` for quickly checking on my own jobs. \n",
    "\n",
    "You can similarly do \n",
    "\n",
    "`squeue -p highcore` to check on a specific partition and so on. \n",
    "\n",
    "\n",
    "Lastly, a really useful command to check when your job is scheduled to start, end and other relevant details is \n",
    "\n",
    "`scontrol show jobid ####`\n",
    "\n",
    "This fills in a few seconds after submission, once SLURM has had a chance to schedule the job. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdc7168-74f9-46a4-8d2b-4e0913e7312a",
   "metadata": {},
   "source": [
    "## Bonus: open remote text files with atom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e63db0-2811-4f9c-8c3b-f95043495446",
   "metadata": {},
   "source": [
    "[Atom](https://atom.io/) is a text editor that has a tonne of useful packages and is very customiseable. \n",
    "\n",
    "[One](https://atom.io/packages/hydrogen) of these incredibly useful packages lets you run python code line by line. \n",
    "\n",
    "[Another](https://atom.io/packages/ftp-remote-edit), lets you SFTP more elegantly, and view and edit the text files on a cluster in Atom, just as if they were on your machine. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0820c4a-867d-4522-82f7-96d5bca6f29b",
   "metadata": {},
   "source": [
    "To set this up, first download and install [atom](https://atom.io/).\n",
    "\n",
    "Then, from its package manager, get the ftp-remote-edit package. \n",
    "\n",
    "Hitting control+space on your laptop should then prompt you to put in a master password. Don't forget it, I don't know how to help you if you do. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54e792b-d1aa-4865-aa81-174e9bacdabb",
   "metadata": {},
   "source": [
    "Then, on the left \"Remote\" column that comes up, right click and Edit servers. Fill in the details, usually for protocol you'd select SFTP for secure file transfer protocol. Here's an example:\n",
    "\n",
    "![ftp_remote_edit](ftp_remote_edit_example.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19368b30-bb31-4c95-b785-418994238ba4",
   "metadata": {},
   "source": [
    "And voila, you're done. \n",
    "\n",
    "You can now access remote files and edit them with ease locally. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
