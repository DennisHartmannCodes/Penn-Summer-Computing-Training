{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a61350e8-30e9-45a6-a82d-7368456ea3f7",
   "metadata": {},
   "source": [
    "# Cluster Computing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e6f443-eaf7-4722-8da3-0460d4ede506",
   "metadata": {},
   "source": [
    "Physics and astronomy often invove large datasets or a large number of computations or both, for which simply running code on your personal machines will not suffice. \n",
    "You could need more firepower (eg. more cores), more memory or just different processors (eg. GPUs). This is where computer clusters come in. You take your code, stick it on a cluster and submit jobs to be run on the cluster remotely. Here, we'll go over the basics of how to do that. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e296405-7f4e-4a63-a4ed-564c73b624a4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Connecting to a cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1530327a-0b7d-4a35-a91b-72de4345684c",
   "metadata": {},
   "source": [
    "In [Section 2](https://github.com/jeffiuliano/Penn-Summer-Computing-Training/blob/main/2_ssh_and_scp/SSH_SCP_Workbook.ipynb), we learnt to securely interact with a non-local machine. To connect to most clusters, we will use SSH as described there. For example, for those of you using *marmalade*, you will do "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221ae077-3e1c-4075-b605-fb43f8a1e731",
   "metadata": {},
   "source": [
    "`ssh username@marmalade.physics.upenn.edu`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2ab7db-f3a8-4bcc-aaa0-950e389771a2",
   "metadata": {},
   "source": [
    "- Note that to connect to *marmalade*, you must be on a Penn secured network (eg. AirPennNet wi-fi) or VPN into Penn. For using the Penn VPN, you need a PennKey (UPenn username and account) and associated account setup. For details of how to VPN into Penn, go [here](https://www.isc.upenn.edu/how-to/university-vpn-getting-started-guide). \n",
    "- Note also that *marmalade* access is granted by your PI. Please bother them to get access. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3851fef5-b11c-4be4-bbd5-d0ad5f6613f2",
   "metadata": {},
   "source": [
    "This will then prompt you for a password. The password for your account should be the same as the password associated with your PennKey. Remember that your terminal will not indicate that you are typing while you enter your password. If you are successful, the terminal will print out something like:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0283683a-183c-4344-a255-b0f8c0c1cb8b",
   "metadata": {},
   "source": [
    "`Last login: Wed May 4 15:58:52 2022 from xx.xxx.xxx.xxx`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8023ef92-b8fb-4da5-88a9-b666102b40bc",
   "metadata": {},
   "source": [
    "You are now on the **head node / login node** on *marmalade*. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0720fe8-307b-43b3-8f78-f0fd66cbb7ac",
   "metadata": {},
   "source": [
    "### Head / Login Nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587f2db3-e2aa-4dbe-a8a5-4dbd005838cd",
   "metadata": {},
   "source": [
    "#### What is a head node and why will everyone get mad at you if you mess with it?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd03c78-ec1f-4cf1-8c0b-e1699c0fbf1d",
   "metadata": {},
   "source": [
    "Most clusters are set up such that you land on a *head or login node* when you connect to it. This is not the location where actual jobs are run, this node will not perform your computations. It's purpose is to facilitate those computations being run on a *compute node*. **Do not use the login node to run scripts.** You might wind up preventing other people from logging in by consuming resources. This node will not have enough memory to support large jobs. Etc etc. **This is how you make everyone in your research group and beyond angry with you.** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe48968-c052-4319-94a5-351e3c45893b",
   "metadata": {},
   "source": [
    "Things it is okay to use the login node for:\n",
    "- view and edit scripts\n",
    "- view output\n",
    "- perform git actions \n",
    "- submit and managae jobs\n",
    "- spy on who else is using the cluster resources "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a9f1e6-6805-4430-b8f9-0619fd2cae39",
   "metadata": {},
   "source": [
    "Things I will yell at you for using the login node for:\n",
    "- running jobs\n",
    "- debugging scripts\n",
    "- I'll probably add some more things here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310c56d4-46d1-4fa3-b3e4-844cdeb80379",
   "metadata": {},
   "source": [
    "Grey area:\n",
    "- managing your python environment\n",
    "- installing other packages \n",
    "- installing code "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68ef391-dd63-4382-b07b-78779eb4fa17",
   "metadata": {},
   "source": [
    "Note that for the \"grey area\" points, it's still better to perform these actions on a compute node if you can, because ultimately, those are the nodes that will be running your jobs, not your login node. For example, *marmalade* has AMD nodes as well as Intel nodes. Use the specific compute nodes that you will run your jobs on to compile your code, because there are some differences between the two types of nodes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f58324-3dbf-40f2-b72e-f2083fe10cbe",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Cores vs. Nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d19585-aaed-488b-a5ac-8038232d2545",
   "metadata": {},
   "source": [
    "**Nodes** are effectivly a self-contained CPU. This has some memory, input/output and storage. This also has processors. The processors are sometimes referred to as **cores**, but usually, each processor is made up of a ~couple cores. \n",
    "\n",
    "The cores then share everything the node has - they share memory, I/O and storage. But for parallelisable code, you can parallelise across these cores and make them simultaneously run tasks. \n",
    "\n",
    "For *marmalade*, \n",
    "- astro has 3 AMD nodes which each have 64 processors or a total of 128 cores\n",
    "- CM group has Intel nodes, you should bother them for their details \n",
    "- CM also has some GPUs "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b7b13d-112c-4e73-b270-7d59af970b53",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Home "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881f5266-781c-43fe-8364-c910136bd1e2",
   "metadata": {},
   "source": [
    "The login node will take you to your home directory. This is where you can install your code, edit your `.bashrc`, direct your output. The home directories on *marmalade* are backed up every 24h. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b28797-3d98-491e-b870-fbad958ea5d9",
   "metadata": {},
   "source": [
    "### Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef8b186-d177-4db3-a3ce-d507cc18e192",
   "metadata": {},
   "source": [
    "Depending on the cluster, a separate location is preferred for runtime output called *scratch*. This has faster input/output than the *home* directory. Depending on how often you need to read/write during a job, it's better to send stuff to scratch as opposed to home. \n",
    "\n",
    "For *marmalade*, the scratch directories are attached to the compute nodes. So if your job was running on `node11` say, then the scratch in node11 will hold your output. The jobs I run on *m* are basic MCMCs, don't need fast I/O, so I just write to home. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b4b6a0-7d44-496c-aca5-f9b474fd9ee9",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7146a3-27c1-4b16-97c1-4e18e2b00ee3",
   "metadata": {},
   "source": [
    "Again, depending on the setup of the cluster, sometimes a data directory is provided per group to share and store large quantities of data. \n",
    "\n",
    "Astro on *m* doesn't have this, but CM does I think. Bother your CM grad students / postdocs to learn more. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3804f67-dcf1-4113-aad8-a68b777b82e4",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Cluster set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90fcb91-b84c-4dc7-b234-8fce4d4d4151",
   "metadata": {},
   "source": [
    "Now that you're logged in, how do you go about setting up you code and ensuring it runs correctly? There's a few simple places to start:\n",
    "- if it's your own, personal code, you can scp it onto the cluster \n",
    "- git clone from a repository "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e94f99e-2069-44d2-b6dd-7200910f6bd2",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### rsync / sftp "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5328194-ba90-4251-b17c-e6121ba0388e",
   "metadata": {},
   "source": [
    "These work similar to the predescribed scp, with some useful differences. \n",
    "\n",
    "\n",
    "`rsync` synchronises files between the two endpoints. It functions similarly to scp - you tell it where you want to transfer the file(s) from and to. Then, rsync will just copy the parts of the file(s) that are different, saving time on large files. Adding `-aP` adds a neat progress bar. Example:\n",
    "\n",
    "`rsync -aP \"user@cluster.location:/home/user/location/of/files/*\" /local/machine/directory/`\n",
    "\n",
    "Note the quotation marks around the cluster address. This is necessary for iTerm or Mac M1s (I forget which) to correctly read in that entire string for the cluster location.\n",
    "\n",
    "\n",
    "`sftp` stands for SSH file transfer protocol. It's a little different from rsync and scp. This lets you SSH into a remote machine and move around in its directories as well as local directories and simply get or put files in either location. It's great for moving files around both ways in different locations. Example:\n",
    "\n",
    "`ssh user@cluster.location`\n",
    "\n",
    "`cd some/remote/directory`\n",
    "\n",
    "`get some_remote_file`\n",
    "\n",
    "`lcd some/local/dorectory`\n",
    "\n",
    "`put some_local_file`\n",
    "\n",
    "Here, cd is change remote directory while lcd is local change directory, same for pwd and lpwd. The command get downloads a remote file to the local working directory, while put uploads a local file to the remote working directory. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16067436-3c17-4e65-9742-7500ff547cb6",
   "metadata": {},
   "source": [
    "### Loading available software"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978e28be-6ad9-4c96-9800-40ad4c9104c6",
   "metadata": {},
   "source": [
    "You might also need modules to be able to run your code, for eg. an installation of C++ or fortran or python. Most clusters will have installations of these already in place, available for you to use. A good place to start to look for these is \n",
    "\n",
    "`module avail` \n",
    "\n",
    "This prints out all available pre-installed software that you can just load onto your profile on the cluster, for eg. with \n",
    "\n",
    "`module load git/xx.xx`\n",
    "\n",
    "You can then check that you indeed have access to git now with \n",
    "\n",
    "`which git`\n",
    "\n",
    "This should print some location of where the git you are using lives `...bin/git`\n",
    "\n",
    "Useful modules on *m* include a few versions of GCC, git, anaconda, valgrind ... The cluster helpdesk (marmalade-manager@sas.upenn.edu) is usually responsive and will install more software to add to the list of available modules if you make a good case for it (perhaps even if you make a bad one). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c2d521-edda-435b-8149-aeabc812b20b",
   "metadata": {},
   "source": [
    "### Saving cluster set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0493c3be-bb3c-485e-8cf3-edb0b2f508fe",
   "metadata": {},
   "source": [
    "Once you know which modules you will need and have compiled your codes based on these modules, you should save this set-up. \n",
    "Check whihc modules you currently have loaded by doing \n",
    "\n",
    "`module list`\n",
    "\n",
    "Then save these in your .bashrc or your .bash_profile to ensure they are loaded everytime you login:\n",
    "\n",
    "`module load xxx/xxx.xx`\n",
    "\n",
    "You should also save your anaconda environment. Someone else will tell you about that. This saves the version numbers of your python packages and loads to exact right ones for which you have compiled your code and know that it works. To export the package info for the active environment, do \n",
    "\n",
    "`conda env export > environment.yml`\n",
    "\n",
    "These are important programming practices and will save you SO much time when things invariably break or you have to move your work to a different cluster. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9554a99c-5a5b-4c4a-aa37-eafdcebf0bc9",
   "metadata": {},
   "source": [
    "### Source cluster set-up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd4ef58-ce21-492f-9f52-f446d9f225e6",
   "metadata": {},
   "source": [
    "When you login, the cluster usually sources your .bashrc and .bash_profile. This loads all your settings, as long as oyu saved them here. \n",
    "\n",
    "Some clusters are set up such that you don't need to reload these settings when you move to a compute node to run a job interactively or when you submit a job. Some aren't. \n",
    "\n",
    "For *m*, I need to source my .bashrc and .bash_profile everytime I run an interactive job, and have gotten into the habit of doing the same for submitted jobs\n",
    "\n",
    "`source .bashrc` \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb22c36d-d6d7-4100-bc2b-21f71f35a1c2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Running jobs "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43e3de7-f577-4a60-90bf-22ddbb2c1b78",
   "metadata": {},
   "source": [
    "Ideally, you'd like to test your code before try to submit a job and have to wait for it to begin, compute and possibly fail because of some bug. Clusters often have debug queues dedicated to this purpose that you can submit jobs to or directly interact with to run your code. \n",
    "\n",
    "Below I'll cover how to do both assuming the cluster uses the SLURM queuing system. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a28c369-6910-4dbd-9191-949ba43fee46",
   "metadata": {},
   "source": [
    "### Interactive sessions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8063d9-2b6a-4b84-888a-f65c9e245ad0",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "`srun -n 4 -p highcore -t 180 --pty bash` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e30eec6-1784-4a6a-a770-06c7de921b15",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ec281c86-79f8-4dd8-a988-8ab2b3150716",
   "metadata": {},
   "source": [
    "### Submitting jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca5b06f-4fb9-4509-b263-3c0f2ebe03f0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8c5b0a31-149d-4e78-8b8a-04fdb1268fa5",
   "metadata": {},
   "source": [
    "#### Which queue is for you? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ae8ebf-d459-43c1-8fb8-b4229399d21a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "09a466bf-4f3c-4dd7-8778-6130a76a95af",
   "metadata": {},
   "source": [
    "#### Submitting batch jobs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b806cbfb-b48b-481e-9dd1-99bbd3f8927f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b3cd0e2c-4722-4c38-be7e-7115431bd984",
   "metadata": {},
   "source": [
    "## Useful SLURM commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffae9c2d-ce02-4962-a34a-adb538d981bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4bdc7168-74f9-46a4-8d2b-4e0913e7312a",
   "metadata": {},
   "source": [
    "## Bonus: open remote text files with atom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68898c19-99eb-47cb-a3cf-20aa9f86b6dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
